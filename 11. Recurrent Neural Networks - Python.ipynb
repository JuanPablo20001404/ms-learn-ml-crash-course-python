{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "11. Recurrent Neural Networks - Python.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanPablo20001404/ms-learn-ml-crash-course-python/blob/master/11.%20Recurrent%20Neural%20Networks%20-%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "a4zHKTuHFdCk"
      },
      "source": [
        "Exercise 11 - Recurrent Neural Networks\n",
        "========\n",
        "\n",
        "A recurrent neural network (RNN) is a class of neural network that excels when your data can be treated as a sequence - such as text, music, speech recognition, connected handwriting, or data over a time period. \n",
        "\n",
        "RNN's can analyse or predict a word based on the previous words in a sentence - they allow a connection between previous information and current information.\n",
        "\n",
        "This exercise looks at implementing a LSTM RNN to generate new characters after learning from a large sample of text. LSTMs are a special type of RNN which dramatically improves the model’s ability to connect previous data to current data where there is a long gap.\n",
        "\n",
        "We will train an RNN model using a novel written by H. G. Wells - The Time Machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDKqq0_sFdCq"
      },
      "source": [
        "Step 1\n",
        "------\n",
        "\n",
        "Let's start by loading our libraries and text file. This might take a few minutes.\n",
        "\n",
        "#### Run the cell below to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "jfRI6btTFdCs"
      },
      "source": [
        "%%capture\n",
        "# Run this!\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "import numpy as np\n",
        "import random, sys, io, string"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bDqc8ybFdC3"
      },
      "source": [
        "#### Replace the `<addFileName>` with `The Time Machine`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HHWB4ACG8sV",
        "outputId": "95d51480-0ee0-4019-a258-554c75550270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "6WpDjrI6FdC5",
        "outputId": "5391c95b-efff-41e7-a982-19e9398b100e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "###\n",
        "# REPLACE THE <addFileName> BELOW WITH The Time Machine\n",
        "###\n",
        "text = io.open('/content/drive/My Drive/Colab Notebooks/Regresión/Data/The Time Machine.txt', encoding = 'UTF-8').read()\n",
        "###\n",
        "\n",
        "# Let's have a look at some of the text\n",
        "print(text[0:198])\n",
        "\n",
        "# This cuts out punctuation and make all the characters lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
            "text length: 174201 characters\n",
            "unique characters: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg_0CE4nFdDA"
      },
      "source": [
        "Expected output:  \n",
        "```The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
        "text length: 174201 characters\n",
        "unique characters: 39```\n",
        "\n",
        "Step 2\n",
        "-----\n",
        "\n",
        "Next we'll divide the text into sequences of 40 characters.\n",
        "\n",
        "Then for each sequence we'll make a training set - the following character will be the correct output for the test set.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `40`\n",
        "#### 2. `<step>` with `4`\n",
        "#### and then __run the code__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "SnNhFrm0FdDC",
        "outputId": "0538cf2b-4847-464c-9065-4cd7d1fc4cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "###\n",
        "# REPLACE <sequenceLength> WITH 40 AND <step> WITH 4\n",
        "###\n",
        "sequence_length = 40\n",
        "step = 4\n",
        "###\n",
        "\n",
        "sequences = []\n",
        "target_chars = []\n",
        "for i in range(0, len(text) - sequence_length, step):\n",
        "    sequences.append([text[i: i + sequence_length]])\n",
        "    target_chars.append(text[i + sequence_length])\n",
        "print('number of training sequences:', len(sequences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training sequences: 43541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk2R0XPQFdDK"
      },
      "source": [
        "Expected output:\n",
        "`number of training sequences: 43541`\n",
        "\n",
        "#### Replace `<addSequences>` with `sequences` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Xcb_RFyNFdDL"
      },
      "source": [
        "# One-hot vectorise\n",
        "\n",
        "X = np.zeros((len(sequences), sequence_length, len(charset)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(charset)), dtype=np.bool)\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSequences> BELOW WITH sequences\n",
        "###\n",
        "for n, sequence in enumerate(sequences):\n",
        "###\n",
        "    for m, character in enumerate(list(sequence[0])):\n",
        "        X[n, m, index_from_char[character]] = 1\n",
        "    y[n, index_from_char[target_chars[n]]] = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chjUOu4LFdDT"
      },
      "source": [
        "Step 3\n",
        "------\n",
        "\n",
        "Let's build our model, using a single LSTM layer of 128 units. We'll keep the model simple for now, so that training does not take too long.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addLSTM>` with `LSTM`\n",
        "#### 2. `<addLayerSize>` with `128`\n",
        "#### 3. `<addSoftmaxFunction>` with `'softmax`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "AwC6QJM3FdDU"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "###\n",
        "# REPLACE THE <addLSTM> BELOW WITH LSTM (use uppercase) AND <addLayerSize> WITH 128\n",
        "###\n",
        "model.add(LSTM(128, input_shape = (X.shape[1], X.shape[2])))\n",
        "###\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSoftmaxFunction> with 'softmax' (INCLUDING THE QUOTES)\n",
        "###\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "###\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgMHs5IhFdDd"
      },
      "source": [
        "The code below generates text at the end of an epoch (one training cycle). This allows us to see how the model is performing as it trains. If you're making a large neural network with a long training time it's useful to check in on the model as see if the text generating is legible as it trains, as overtraining may occur and the output of the model turn to nonsense.\n",
        "\n",
        "The code below will also save a model if it is the best performing model, so we can use it later.\n",
        "\n",
        "#### Run the code below, but don't change it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "1EO7hVkMFdDe"
      },
      "source": [
        "# Run this, but do not edit.\n",
        "# It helps generate the text and save the model epochs.\n",
        "\n",
        "# Generate new text\n",
        "def on_epoch_end(epoch, _):\n",
        "    diversity = 0.5\n",
        "    print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "    start = random.randint(0, len(text) - sequence_length - 1)\n",
        "    seed = text[start: start + sequence_length]\n",
        "    print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "    output = seed[:40].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    print(output, end = '')\n",
        "\n",
        "    for i in range(500):\n",
        "        x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "        for t, char in enumerate(output):\n",
        "            x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "        exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "        next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "        next_char = char_from_index[next_index]\n",
        "\n",
        "        output = output[1:] + next_char\n",
        "\n",
        "        print(next_char, end = '')\n",
        "    print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "# Save the model\n",
        "checkpoint = ModelCheckpoint('Models/model-epoch-{epoch:02d}.hdf5', \n",
        "                             monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDzUpkDsFdDn"
      },
      "source": [
        "The code below will start the model to train. This may take a long time. Feel free to stop the training with the `square stop button` to the right of the `Run button` in the toolbar.\n",
        "\n",
        "Later in the exercise, we will load a pretrained model.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addPrintCallback>` with `print_callback`\n",
        "#### 2. `<addCheckpoint>` with `checkpoint`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "2ZwbKsv-FdDp",
        "outputId": "214133ba-bdaa-4925-9d9e-bf3cfa8f9672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addPrintCallback> WITH print_callback AND <addCheckpoint> WITH checkpoint\n",
        "###\n",
        "model.fit(X, y, batch_size = 128, epochs = 3, callbacks = [print_callback, checkpoint])\n",
        "###"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.7714\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"ess that i thought chiefly of the philos\"\n",
            "ess that i thought chiefly of the philos the the s n  the fe ton inl se ghe icce the ftf the le ahe in shennas atreute athee t than te an the lot to t d s he e ithe ine  in  te thee the t ae an the et ere mhe the i a the ne then to weme dard gn the sa the cea ane their  ane tome sn ther in ind nat an soran toi tiin ithe ihe tinhe aed in e the dol to ofe fed the toe sn the ais th inle the thi tere the t ah he s ter ulatt then sin th ian t un s r the then iau n e s see teon d ite te wai tio tpt ihe a nint th nn tiher an toet ahe tiin an\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.77124, saving model to Models/model-epoch-01.hdf5\n",
            "341/341 [==============================] - 56s 164ms/step - loss: 2.7712\n",
            "Epoch 2/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.3817\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"de was along the timedimension”  “but” s\"\n",
            "de was along the timedimension”  “but” s ao the eas an and an of mo ins and the s ois  of mad so hof ind the po mo the ma at tout on on an ang on the in an aa fan as an and wa the san to me the and than out an an the ton of the the the the the lor an not the the sore the ind sa the the the s il the the the the the we ang in the the ind wa lend was and in an the an an of the whe an she to the thi wor the t on in in the se sand ong and th the the the the more the tor t in cos the sas the the on the thas hat on the b an tat on the the th\n",
            "\n",
            "Epoch 00002: loss improved from 2.77124 to 2.38195, saving model to Models/model-epoch-02.hdf5\n",
            "341/341 [==============================] - 57s 167ms/step - loss: 2.3820\n",
            "Epoch 3/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.2382\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"retty laughing faces it was a foolish im\"\n",
            "retty laughing faces it was a foolish ime me lace there ce the the and to mate she the ans or the we me the tho int i the bes ar out on led ale plat the thing are souls and wind the in the the the be the mere med they and ind dat and the ass ant the lere the med the lame too the same the mo peter and ine ond was the the ther the lof mas sopling wat the ind and the the the the t on the the the the certim the  e the tere the sore the the the the the the the the t ore the sous in the the the the the the seras me pame wand sor let ald the\n",
            "\n",
            "Epoch 00003: loss improved from 2.38195 to 2.23800, saving model to Models/model-epoch-03.hdf5\n",
            "341/341 [==============================] - 57s 166ms/step - loss: 2.2380\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb27708cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AQYtBnwFdDy"
      },
      "source": [
        "The output won't appear to be very good. But then, this dataset is small, and we have trained it only for a short time using a rather small RNN. How might it look if we upscaled things?\n",
        "\n",
        "Step 5\n",
        "------\n",
        "\n",
        "We could improve our model by:\n",
        "* Having a larger training set.\n",
        "* Increasing the number of LSTM units.\n",
        "* Training it for longer\n",
        "* Experimenting with difference activation functions, optimization functions etc\n",
        "\n",
        "Training this would still take far too long on most computers to see good results - so we've trained a model already for you.\n",
        "\n",
        "This model uses a different dataset - a few of the King Arthur tales pasted together. The model used:\n",
        "* sequences of 50 characters\n",
        "* Two LSTM layers (512 units each)\n",
        "* A dropout of 0.5 after each LSTM layer\n",
        "* Only 30 epochs (we'd recomend 100-200)\n",
        "\n",
        "Let's try importing this model that has already been trained.\n",
        "\n",
        "#### Replace `<addLoadModel>` with `load_model` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "NJ81N4utFdD0",
        "outputId": "7d87dd11-b2bb-41c7-dd3c-f72ae1f39729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "print(\"loading model... \", end = '')\n",
        "\n",
        "###\n",
        "# REPLACE <addLoadModel> BELOW WITH load_model\n",
        "###\n",
        "model = load_model('/content/drive/My Drive/Colab Notebooks/Regresión/Models/arthur-model-epoch-30.hdf5')\n",
        "###\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
        "###\n",
        "\n",
        "print(\"model loaded\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model... model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4drsWRBrFdD7"
      },
      "source": [
        "Step 6\n",
        "-------\n",
        "\n",
        "Now let's use this model to generate some new text!\n",
        "\n",
        "#### Replace `<addFilePath>` with `'Data/Arthur tales.txt'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Da4qzaWkFdD8",
        "outputId": "6617e648-12f1-4fb5-bd94-12af82cc0693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addFilePath> BELOW WITH 'Data/Arthur tales.txt' (INCLUDING THE QUOTATION MARKS)\n",
        "###\n",
        "text = io.open(\"/content/drive/My Drive/Colab Notebooks/Regresión/Data/Arthur tales.txt\", encoding='UTF-8').read()\n",
        "###\n",
        "\n",
        "# Cut out punctuation and make lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text length: 3645951 characters\n",
            "unique characters: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpmOCUdeFdEF"
      },
      "source": [
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `50`\n",
        "#### 2. `<writeSentence>` with a sentence of your own, at least 50 characters long.\n",
        "#### 3. `<numCharsToGenerate>` with the number of characters you want to generate (choose a large number, like 1500)\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "aVDCx7H7FdEG",
        "outputId": "aabda566-6a2a-452c-d33c-bcf913b563f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "# Generate text\n",
        "\n",
        "diversity = 0.5\n",
        "print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "###\n",
        "# REPLACE <sequenceLength> BELOW WITH 50\n",
        "###\n",
        "sequence_length = 50\n",
        "###\n",
        "\n",
        "# Next we'll make a starting point for our text generator\n",
        "\n",
        "###\n",
        "# REPLACE <writeSentence> WITH A SENTENCE OF AT LEAST 50 CHARACTERS\n",
        "###\n",
        "seed = \"Vale of Avilion to heal me of my grievous wound, and if ye see me no more, pray for my soul.\"\n",
        "###\n",
        "\n",
        "seed = seed.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "###\n",
        "# OR, ALTERNATIVELY, UNCOMMENT THE FOLLOWING TWO LINES AND GRAB A RANDOM STRING FROM THE TEXT FILE\n",
        "###\n",
        "\n",
        "#start = random.randint(0, len(text) - sequence_length - 1)\n",
        "#seed = text[start: start + sequence_length]\n",
        "\n",
        "###\n",
        "\n",
        "print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "output = seed[:sequence_length].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(output, end = '')\n",
        "\n",
        "###\n",
        "# REPLACE THE <numCharsToGenerate> BELOW WITH THE NUMBER OF CHARACTERS WE WISH TO GENERATE, e.g. 1500\n",
        "###\n",
        "for i in range(1500):\n",
        "###\n",
        "    x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "    for t, char in enumerate(output):\n",
        "        x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "    predictions = model.predict(x_pred, verbose=0)[0]\n",
        "    exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "    next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "    next_char = char_from_index[next_index]\n",
        "\n",
        "    output = output[1:] + next_char\n",
        "\n",
        "    print(next_char, end = '')\n",
        "print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"vale of avilion to heal me of my grievou\"\n",
            "vale of avilion to heal me of my grievous wound and all the queen that thou didst not well be a knight and therefore the queen said sir launcelot i deem it well said sir gawaine wit ye well sir tristram i will be sorely suffered for this miserance as i see my brother sir tristram  then sir launcelot was being there and when he had done they were known that it was sir palomides and sir palomides and sir gawaine she had lived and so he sent him off his helm and there sir launcelot was wroth with his two beasts and well said the king said the king that i will make herald man and said the knight that i will that he would not be without a castle whereat sir tristram rode on their horses in the castle  and at the last they were the same window and the chamber  and the hermit day beside the castle and the king was the son of your and good damsel so that it was the chief of all the house when they were departed and then all the people went to the palace of the south  and the next day they were welcomed the strange knight sir palomides and sir gareth de ganis with the helmet and when he fought with the horses and the book sail when they saw him he said fair lady ye shall assay you thou wilt that i may not have been done to do thee i will for i see the page of no me and the maiden were to slay the world to the earth the figure of a priest  and as they came to the castle and as he came forth and the ki"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a958aac3fa3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_from_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mexp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_preds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihrFMisBFdEO"
      },
      "source": [
        "How does it look? Does it seem intelligible?\n",
        "\n",
        "Conclusion\n",
        "--------\n",
        "\n",
        "We have trained an RNN that learns to predict characters based on a text sequence. We have trained a lightweight model from scratch, as well as imported a pre-trained model and generated new text from that."
      ]
    }
  ]
}